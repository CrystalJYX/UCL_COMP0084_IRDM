{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38c7e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.autograd import Variable\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer \n",
    "import collections\n",
    "from collections import Counter\n",
    "import random\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e765b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format : qid pid queries passage relevancy\n",
    "# 1103039 rows × 5 columns\n",
    "validation_data = pd.read_csv('validation_data.tsv', sep='\\t',header=0,low_memory=False)\n",
    "\n",
    "# 4364339 rows × 5 columns\n",
    "train_data = pd.read_csv('train_data.tsv', sep='\\t',header=0,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f4f697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, stopword_removal = False, lemma = False):\n",
    "    \"\"\"\n",
    "        A text preprocessing function\n",
    "        Inputs:\n",
    "          text: input queries/passages\n",
    "          stopword_removal: remove all stopwords if True\n",
    "          lemma: do lemmatisation and stemming if True\n",
    "        Outputs:\n",
    "          passage: queries/passages after preprocessing\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    passage = []\n",
    "    for i in range(len(text)):\n",
    "        words = text[i].lower()\n",
    "        # remove punctuation\n",
    "        rm_punc =re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        words = rm_punc.sub('', words)\n",
    "        # remove all the numbers\n",
    "        words = re.sub(r'[^a-zA-Z\\s]', u' ', words, flags=re.UNICODE)\n",
    "        # tokenize\n",
    "        token_words = word_tokens.tokenize(words)\n",
    "        \n",
    "        # stop word removal\n",
    "        if (stopword_removal == True):\n",
    "            token_words = [w for w in token_words if not w in stop_words]\n",
    "        \n",
    "        sentence = []\n",
    "        # lemmatisation & stemming\n",
    "        if (lemma == True):\n",
    "            stemmer = SnowballStemmer('english')\n",
    "            for i in token_words:      \n",
    "                sentence.append(stemmer.stem(i))\n",
    "        else:\n",
    "            sentence = token_words\n",
    "        passage.append(sentence) \n",
    "    return passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f34d97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relevant_dict(data):\n",
    "    \"\"\"\n",
    "        A relevant and irrelevant passage function\n",
    "        Inputs:\n",
    "          data: input dataset\n",
    "        Outputs:\n",
    "          relevant_dict: relevant passage dictionary with a format of {qid: {pid, position}}\n",
    "          irrelevant_dict: irrelevant passage dictionary with a format of {qid: {pid, position}}\n",
    "    \"\"\"\n",
    "    qid_list = data.qid\n",
    "    pid_list = data.pid\n",
    "    relevancy_list = data.relevancy\n",
    "    relevant_dict = {}\n",
    "    irrelevant_dict = {}\n",
    "    for ind,qid in enumerate(qid_list):\n",
    "        pid = pid_list[ind]\n",
    "        relevancy = relevancy_list[ind]\n",
    "        if relevancy > 0:\n",
    "            if qid not in relevant_dict.keys():\n",
    "                relevant_dict[qid] = {pid:ind}\n",
    "            elif qid in relevant_dict.keys():\n",
    "                new_pid = {pid:ind}\n",
    "                relevant_dict[qid].update(new_pid)\n",
    "        else:\n",
    "            if qid not in irrelevant_dict.keys():\n",
    "                irrelevant_dict[qid] = {pid:ind}\n",
    "            elif qid in irrelevant_dict.keys():\n",
    "                new_pid = {pid:ind}\n",
    "                irrelevant_dict[qid].update(new_pid)\n",
    "\n",
    "    return relevant_dict,irrelevant_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2795c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_relevant_dict, valid_irrelevant_dict = Relevant_dict(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "004867c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_relevant_dict, train_irrelevant_dict = Relevant_dict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "586e641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsampling(data):\n",
    "    \"\"\"\n",
    "        A subsampling function\n",
    "        Inputs:\n",
    "          data: input dataset\n",
    "        Outputs:\n",
    "           dataset after negative down sampling\n",
    "    \"\"\"\n",
    "    # a list store all subsamples' positions selected\n",
    "    DF_list = []\n",
    "    \n",
    "    # for each query\n",
    "    for qid in train_relevant_dict.keys():   \n",
    "        \n",
    "        # keep all relevant passage, record their positions\n",
    "        rel_list = list(train_relevant_dict[qid].values())\n",
    "        \n",
    "        # random choose samples from irrelevant passage with a rate of 0.025, \n",
    "        # record their positions\n",
    "        if qid not in train_irrelevant_dict.keys():\n",
    "            irrel_list = []\n",
    "            \n",
    "        else:\n",
    "            L = list(train_irrelevant_dict[qid].values())\n",
    "            \n",
    "            # if the number of irrelevant passages for this qid is samller than 25, \n",
    "            # keep all irrelevant passages\n",
    "            if len(L) <= 5:\n",
    "                irrel_list = L\n",
    "                \n",
    "            # if the number of irrelevant passages for this qid is larger than 25,\n",
    "            # choose them by the rate of 0.025\n",
    "            else:\n",
    "                irrel_list = random.sample(L,5) \n",
    "                # choose 25 here, since most amount of irrelevant passages is around 1000\n",
    "                # 1000*0.025 = 25\n",
    "        \n",
    "        sample_ind = rel_list + irrel_list\n",
    "        DF_list += sample_ind  \n",
    "    \n",
    "    # convert positions to their corresponding rows\n",
    "    NewData = []\n",
    "    for i in DF_list:\n",
    "        newdata = data[i:i+1]\n",
    "        NewData.append(newdata)\n",
    "    \n",
    "    # merge all the subsamples and convert to a dataFrame\n",
    "    return pd.concat(NewData,axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e68702a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>queries</th>\n",
       "      <th>passage</th>\n",
       "      <th>relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>709560</td>\n",
       "      <td>1050990</td>\n",
       "      <td>what is all in basic metabolic panel</td>\n",
       "      <td>Basic Metabolic Panel. The basic metabolic pan...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>709560</td>\n",
       "      <td>8695294</td>\n",
       "      <td>what is all in basic metabolic panel</td>\n",
       "      <td>This gives you the basic instructions on how a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>709560</td>\n",
       "      <td>1050988</td>\n",
       "      <td>what is all in basic metabolic panel</td>\n",
       "      <td>A Dr. Kathleen Handal, MD , Emergency Medicine...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>709560</td>\n",
       "      <td>2901427</td>\n",
       "      <td>what is all in basic metabolic panel</td>\n",
       "      <td>Calories are the basic unit of energy found in...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>709560</td>\n",
       "      <td>2415797</td>\n",
       "      <td>what is all in basic metabolic panel</td>\n",
       "      <td>(See also Diabetes Mellitus.) Diabetic ketoaci...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27721</th>\n",
       "      <td>969974</td>\n",
       "      <td>2569623</td>\n",
       "      <td>where did the the trail of tears end</td>\n",
       "      <td>The Cherokees presented their own memorial to ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27722</th>\n",
       "      <td>969974</td>\n",
       "      <td>1054561</td>\n",
       "      <td>where did the the trail of tears end</td>\n",
       "      <td>The museum is located along the route traveled...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27723</th>\n",
       "      <td>969974</td>\n",
       "      <td>6724015</td>\n",
       "      <td>where did the the trail of tears end</td>\n",
       "      <td>This challenging 2.1 mile trail connects the u...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27724</th>\n",
       "      <td>969974</td>\n",
       "      <td>778287</td>\n",
       "      <td>where did the the trail of tears end</td>\n",
       "      <td>c. tears The act of weeping: criticism that le...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27725</th>\n",
       "      <td>969974</td>\n",
       "      <td>218181</td>\n",
       "      <td>where did the the trail of tears end</td>\n",
       "      <td>Newton Blackmour State Trail New London The Ne...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27726 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid      pid                               queries  \\\n",
       "0      709560  1050990  what is all in basic metabolic panel   \n",
       "1      709560  8695294  what is all in basic metabolic panel   \n",
       "2      709560  1050988  what is all in basic metabolic panel   \n",
       "3      709560  2901427  what is all in basic metabolic panel   \n",
       "4      709560  2415797  what is all in basic metabolic panel   \n",
       "...       ...      ...                                   ...   \n",
       "27721  969974  2569623  where did the the trail of tears end   \n",
       "27722  969974  1054561  where did the the trail of tears end   \n",
       "27723  969974  6724015  where did the the trail of tears end   \n",
       "27724  969974   778287  where did the the trail of tears end   \n",
       "27725  969974   218181  where did the the trail of tears end   \n",
       "\n",
       "                                                 passage  relevancy  \n",
       "0      Basic Metabolic Panel. The basic metabolic pan...        1.0  \n",
       "1      This gives you the basic instructions on how a...        0.0  \n",
       "2      A Dr. Kathleen Handal, MD , Emergency Medicine...        0.0  \n",
       "3      Calories are the basic unit of energy found in...        0.0  \n",
       "4      (See also Diabetes Mellitus.) Diabetic ketoaci...        0.0  \n",
       "...                                                  ...        ...  \n",
       "27721  The Cherokees presented their own memorial to ...        0.0  \n",
       "27722  The museum is located along the route traveled...        0.0  \n",
       "27723  This challenging 2.1 mile trail connects the u...        0.0  \n",
       "27724  c. tears The act of weeping: criticism that le...        0.0  \n",
       "27725  Newton Blackmour State Trail New London The Ne...        0.0  \n",
       "\n",
       "[27726 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subdata = subsampling(train_data)\n",
    "train_subdata # 27726 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d57ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtrain_relevant_dict, subtrain_irrelevant_dict = Relevant_dict(train_subdata)\n",
    "#len(subtrain_relevant_dict.keys())   # 4590 qids\n",
    "#len(subtrain_irrelevant_dict.keys())  # 4589 qids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a64a89f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l2/gqnx870j10s9yq3h98czyx8h0000gn/T/ipykernel_2582/752341677.py:10: DeprecationWarning: 'U' mode is deprecated\n",
      "  for vocab_size, line in enumerate(open(txt,'rU')):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"glove.6B.100d.txt\"\n",
    " \n",
    "# Get vector_size\n",
    "with open(txt, 'r') as f:\n",
    "    line = f.readline().split(' ')\n",
    "    vector_size = len(line) - 1\n",
    "    \n",
    "# Get vocab_size\n",
    "vocab_size = -1\n",
    "for vocab_size, line in enumerate(open(txt,'rU')):\n",
    "    pass\n",
    "vocab_size += 1\n",
    " \n",
    "# Add them to the start of file\n",
    "with open(txt, 'r+') as f:\n",
    "    content = f.read()        \n",
    "    f.seek(0, 0)\n",
    "    f.write(('%d %d\\n' % (vocab_size, vector_size)) + content)\n",
    "    \n",
    "word_model = KeyedVectors.load_word2vec_format('glove.6B.100d.txt', binary=False)\n",
    "len(list(word_model.key_to_index))  # 400000 words, 100 dim for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "adf62e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 0\n",
    "for i in tqdm(range(len(qid_query_train.keys()))):\n",
    "    length = len(list(qid_query_train.values())[i])\n",
    "    if length > max_length:\n",
    "        max_length = length \n",
    "for i in tqdm(range(len(pid_passage_train.keys()))):\n",
    "    length = len(list(pid_passage_train.values())[i])\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "for i in tqdm(range(len(qid_query_valid.keys()))):\n",
    "    length = len(list(qid_query_valid.values())[i])\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "for i in tqdm(range(len(pid_passage_valid.keys()))):\n",
    "    length = len(L[i])\n",
    "    if length > max_length:\n",
    "        max_length = length \n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "86abc46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_passages = preprocessing(train_subdata.passage, stopword_removal = True, lemma = False)\n",
    "train_queries = preprocessing(train_subdata.queries, stopword_removal = True, lemma = False)\n",
    "test_passages = preprocessing(validation_data.passage, stopword_removal = True, lemma = False)\n",
    "test_queries = preprocessing(validation_data.queries, stopword_removal = True, lemma = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bb6d1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_table(datasets,model):\n",
    "    token_to_ind = {} # tokens to indexes\n",
    "    ind_to_vec = {} # indexes to word vectors\n",
    "    i = 0\n",
    "    \n",
    "    for dataset in tqdm(datasets):\n",
    "        for sentence in dataset: # for each query/passage\n",
    "            for token in sentence: # for each token of the sentence\n",
    "                # if this word is not token_to_ind\n",
    "                if(token_to_ind.get(token) == None):\n",
    "                    if token in model:\n",
    "                    # if this word exists is the word model\n",
    "                        i += 1\n",
    "                        token_to_ind[token] = i\n",
    "                        ind_to_vec[i] = model[token]\n",
    "\n",
    "    return token_to_ind, ind_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "99ff6749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:12<00:00,  3.15s/it]\n"
     ]
    }
   ],
   "source": [
    "# 150119 words\n",
    "token_ind_dict, ind_vec_dict = word_table([train_passages,train_queries,test_passages,test_queries], word_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "278c769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_sentence_embedding(token_ind_dict,ind_vec_dict,text):\n",
    "    sentence_vec = []\n",
    "  ## for every sentence\n",
    "    for sentence in tqdm(text):\n",
    "        sentence_vec_list = []\n",
    "        for word in sentence:\n",
    "            word_index = token_ind_dict.get(word)\n",
    "        if(word_index!=None):\n",
    "            word_embedding = ind_vec_dict.get(word_index)\n",
    "            sentence_vec_list.append(np.array(word_embedding))\n",
    "        else:\n",
    "            sentence_vec_list.append(np.array(np.zeros(100)))\n",
    "    if len(sentence_vec_list) == 0:\n",
    "        sentence_vec_list.append(np.array(np.zeros(100)))\n",
    "        sentence_vec_list = np.array(sentence_vec_list)\n",
    "\n",
    "    sentence_vec.append(np.mean(sentence_vec_list,axis = 0))\n",
    "\n",
    "    return np.array(sentence_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9c72119f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 27726/27726 [00:01<00:00, 13876.53it/s]\n",
      "100%|██████████████████████████████████| 27726/27726 [00:00<00:00, 34015.40it/s]\n",
      "100%|██████████████████████████████| 1103039/1103039 [01:26<00:00, 12734.22it/s]\n",
      "100%|██████████████████████████████| 1103039/1103039 [00:20<00:00, 53817.00it/s]\n"
     ]
    }
   ],
   "source": [
    "train_passage_ind = new_sentence_embedding(token_ind_dict, ind_vec_dict, train_passages)\n",
    "train_queries_ind= new_sentence_embedding(token_ind_dict, ind_vec_dict, train_queries)\n",
    "\n",
    "test_passage_ind = new_sentence_embedding(token_ind_dict, ind_vec_dict, test_passages)\n",
    "test_queries_ind = new_sentence_embedding(token_ind_dict, ind_vec_dict, test_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a1a54718",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_subdata['relevancy'].values\n",
    "test_labels = validation_data['relevancy'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f576d107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150120, 100)\n"
     ]
    }
   ],
   "source": [
    "def look_up_table(ind_vec_dict):\n",
    "    table = [np.zeros(100)]\n",
    "    for key in sorted (ind_vec_dict.keys()) :  \n",
    "        table.append(ind_vec_dict.get(key))\n",
    "    return np.array(table)\n",
    "\n",
    "new_table = look_up_table(ind_vec_dict)\n",
    "print(new_table.shape) # (150120, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "36e8de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_zeros(max_length,vector):\n",
    "    vector = np.array(vector)\n",
    "    if(vector.shape[0] < max_length and vector.shape[0] != 0):\n",
    "        padding_vector = np.zeros(max_length - vector.shape[0])\n",
    "        return np.concatenate((vector, padding_vector), axis=0)\n",
    "    elif(vector.shape[0] == max_length):\n",
    "        return vector\n",
    "    else:\n",
    "        return np.zeros((0,0))\n",
    "\n",
    "def word_to_index_func(text,max_length,labels,token_ind_dict):\n",
    "    embedding_ind = []\n",
    "    embedding_labels = []\n",
    "    sentence_lengths = []\n",
    "    i = -1\n",
    "\n",
    "    for sentence in tqdm(text):\n",
    "        i += 1\n",
    "        embedding_sentence = []\n",
    "        for word in sentence:\n",
    "            if(token_ind_dict.get(word)!=None):\n",
    "                embedding_sentence.append(token_ind_dict.get(word))\n",
    "\n",
    "    sentence_lengths.append(len(embedding_sentence))\n",
    "\n",
    "    embedding_sentence =  padding_zeros(max_length,embedding_sentence) \n",
    "    embedding_labels.append(labels[i])\n",
    "    \n",
    "    if(embedding_sentence.shape[0]!=0):\n",
    "        embedding_ind.append(np.array(embedding_sentence))\n",
    "    else:\n",
    "        embedding_ind.append(np.zeros(max_length))\n",
    "    Ind = np.array(embedding_ind)\n",
    "    Labels = np.array(embedding_labels)\n",
    "    Len = np.array(sentence_lengths)\n",
    "    return Ind,Labels,Len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8889e2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 27726/27726 [00:01<00:00, 18727.03it/s]\n",
      "100%|██████████████████████████████████| 27726/27726 [00:00<00:00, 83024.69it/s]\n",
      " 77%|███████████████████████▉       | 852886/1103039 [00:45<00:09, 25849.26it/s]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_pInd = torch.from_numpy(train_pInd).int().to(device)\n",
    "train_qInd = torch.from_numpy(train_qInd).int().to(device)\n",
    "train_plen = torch.from_numpy(train_plen)\n",
    "train_plen = torch.as_tensor(train_plen,dtype=torch.int64)\n",
    "train_qlen = torch.from_numpy(train_qlen)\n",
    "train_qlen = torch.as_tensor(train_qlen, dtype=torch.int64)\n",
    "train_labels = torch.from_numpy(train_labels).to(device)\n",
    "\n",
    "new_table = torch.from_numpy(new_table).float().to(device)\n",
    "\n",
    "test_pInd = torch.from_numpy(test_pInd).int().to(device)\n",
    "test_qInd = torch.from_numpy(test_qInd).int().to(device)\n",
    "test_plen = torch.from_numpy(test_plen)\n",
    "test_plen = torch.as_tensor(test_plen, dtype=torch.int64)\n",
    "test_qlen = torch.from_numpy(test_qlen)\n",
    "test_qlen = torch.as_tensor(test_qlen, dtype=torch.int64)\n",
    "test_labels = torch.from_numpy(test_labels).to(device)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_pInd, train_plen, train_qInd, train_qlen, train_labels)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(test_pInd, test_plen, test_qInd, test_queries_lengths, test_qlen)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "357f4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim):\n",
    "        super(RNN,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers = 2,\n",
    "                          bidirectional = True, dropout = 0.5)\n",
    "        self.fc = nn.Linear(hidden_dim*2,1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedding = self.dropout(self,embedding(x))\n",
    "        output,(hidden,cell) = self.rnn(embedding)\n",
    "        hidden - torch.cat(hidden[-2],hidden[-1],dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        out = self.fc(hidden)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "df460591",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(len(look_up_table),100,256)\n",
    "pretrained_embedding = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ba79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, iterator,optimizer,stop):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
